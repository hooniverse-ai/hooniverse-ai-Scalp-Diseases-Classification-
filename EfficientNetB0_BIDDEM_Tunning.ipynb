{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10458ca1",
   "metadata": {},
   "source": [
    "# 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af59c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% 표준 라이브러리\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "# %% 수치·데이터 처리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# %% 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "\n",
    "# %% PyTorch 및 관련\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, OneCycleLR, ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.amp import autocast\n",
    "\n",
    "\n",
    "# %% torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "import optuna\n",
    "\n",
    "# %% 유틸리티\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "# %% 데이터 분할\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# %% 실험 추적\n",
    "import wandb\n",
    "\n",
    "\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows의 경우, 한글 지원 폰트로 설정\n",
    "plt.rcParams['axes.unicode_minus'] = False       # 음수 기호가 깨지지 않도록 설정\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf5b03",
   "metadata": {},
   "source": [
    "# 파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb6325c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    EPOCH = 2\n",
    "    N_TRIALS = 2    \n",
    "else:\n",
    "    EPOCH = 20\n",
    "    N_TRIALS = 20\n",
    "    \n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "DESEASE = 'BIDDEM'\n",
    "model_version = f\"EfficientNet_B0_{DESEASE}_TUNNING\"\n",
    "dataset = \"SCALP\"\n",
    "model_type = model_version + '_' + datetime.now().strftime(\"%Y_%m_%d_%H\")\n",
    "    \n",
    "data_path = rf'C:\\Users\\user1\\Desktop\\Code\\Scalp_Disease_Classifier\\data\\biddem_org_3_preprocess'\n",
    "save_model_path = rf\"C:\\Users\\user1\\Desktop\\Code\\Scalp_Disease_Classifier\\result\\model\\{model_type}_model.pt\"\n",
    "save_history_path = rf\"C:\\Users\\user1\\Desktop\\Code\\Scalp_Disease_Classifier\\result\\history\\{model_type}_history.pt\"\n",
    "save_result_csv_path = rf\"C:\\Users\\user1\\Desktop\\Code\\Scalp_Disease_Classifier\\result\\biddem_model_results.csv\"\n",
    "\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a376c01",
   "metadata": {},
   "source": [
    "# 데이터 셋 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3498c441",
   "metadata": {},
   "source": [
    "## transform 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12e606ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CLAHETransform(object):\n",
    "#     def __init__(self, clip_limit=0.4, grid_size=(4,4)):\n",
    "#         self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)\n",
    "\n",
    "#     def __call__(self, img):\n",
    "#         img_np = np.array(img)\n",
    "#         lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
    "#         l, a, b = cv2.split(lab)\n",
    "#         l_clahe = self.clahe.apply(l)\n",
    "#         lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "#         img_rgb = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)\n",
    "#         return Image.fromarray(img_rgb)\n",
    "\n",
    "# # Sharpen transform\n",
    "# class SharpenTransform(object):\n",
    "#     def __call__(self, img):\n",
    "#         return img.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n",
    "\n",
    "# 3. 학습용 transform\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE, interpolation=4),\n",
    "    transforms.Lambda(lambda x: x.rotate(90)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 4. 테스트/검증용 transform (반사광 제거, 대비 향상 동일하게 적용)\n",
    "test_val_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE, interpolation=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c7e4c3",
   "metadata": {},
   "source": [
    "## 커스텀 데이터 셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b23a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ① 합친 클래스 이름 정의 (3개)\n",
    "custom_order = ['양호', '경증', '중증']\n",
    "\n",
    "class CustomOrderImageFolder(ImageFolder):\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 transform=None,\n",
    "                 target_transform=None,\n",
    "                 custom_order=None):\n",
    "        self.custom_order = custom_order\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    def find_classes(self, directory):\n",
    "        if self.custom_order is not None:\n",
    "            classes = self.custom_order\n",
    "        else:\n",
    "            classes = [d for d in os.listdir(directory)\n",
    "                       if os.path.isdir(os.path.join(directory, d))]\n",
    "            classes.sort()\n",
    "        class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "class CustomSubset(Dataset):\n",
    "    def __init__(self, image_folder, indices, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        path, label = self.image_folder.samples[actual_idx]\n",
    "        image = self.image_folder.loader(path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35e1e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋 (test 폴더에서 전체 샘플 가져옴)\n",
    "full_ds = CustomOrderImageFolder(\n",
    "    root = os.path.join(data_path, 'test'),\n",
    "    transform = None,                  # 무시되므로 None 으로\n",
    "    custom_order = custom_order,\n",
    ")\n",
    "\n",
    "# Stratified Split (앞에서 설명된 대로)\n",
    "labels = np.array([lbl for _, lbl in full_ds.samples])\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "train_idx, test_idx = next(sss.split(np.zeros(len(labels)), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1ff274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50% Subset] train: 14436개, val: 2062개, test: 2062개\n"
     ]
    }
   ],
   "source": [
    "train_DS = CustomOrderImageFolder(\n",
    "    root = os.path.join(data_path, 'train'),\n",
    "    transform = train_transform,\n",
    "    custom_order = custom_order,\n",
    ")\n",
    "\n",
    "val_DS = CustomSubset(\n",
    "    image_folder=full_ds,\n",
    "    indices=train_idx,\n",
    "    transform=test_val_transform\n",
    ")\n",
    "test_DS = CustomSubset(\n",
    "    image_folder=full_ds,\n",
    "    indices=test_idx,\n",
    "    transform=test_val_transform\n",
    ")\n",
    "\n",
    "if DEBUG:\n",
    "    num_train = int(len(train_DS) * 0.1)\n",
    "    num_val   = int(len(val_DS) * 0.1)\n",
    "    train_DS  = Subset(train_DS, list(range(num_train)))\n",
    "    val_DS    = Subset(val_DS,   list(range(num_val)))\n",
    "    print(f\"[DEBUG] train: {len(train_DS)}개, val: {len(val_DS)}개, test: {len(test_DS)}개\")\n",
    "    \n",
    "else:\n",
    "    # 1. train_DS의 라벨 추출\n",
    "    if isinstance(train_DS, Subset):\n",
    "        targets = [train_DS.dataset.samples[i][1] for i in train_DS.indices]\n",
    "    else:\n",
    "        targets = [lbl for _, lbl in train_DS.samples]\n",
    "\n",
    "    # 2. StratifiedShuffleSplit을 활용해 50% 서브셋 인덱스 추출\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "    subset_idx, _ = next(sss.split(np.zeros(len(targets)), targets))\n",
    "    train_DS = Subset(train_DS, subset_idx)\n",
    "\n",
    "    # val_DS도 50%로 축소 (val_DS는 CustomSubset이므로 targets 다르게 추출)\n",
    "    if isinstance(val_DS, Subset):\n",
    "        val_targets = [val_DS.dataset.samples[i][1] for i in val_DS.indices]\n",
    "    else:\n",
    "        val_targets = [full_ds.samples[i][1] for i in train_idx]\n",
    "    sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "    val_subset_idx, _ = next(sss_val.split(np.zeros(len(val_targets)), val_targets))\n",
    "    val_DS = Subset(val_DS, val_subset_idx)\n",
    "\n",
    "    # test_DS도 50%로 축소\n",
    "    if isinstance(test_DS, Subset):\n",
    "        test_targets = [test_DS.dataset.samples[i][1] for i in test_DS.indices]\n",
    "    else:\n",
    "        test_targets = [full_ds.samples[i][1] for i in test_idx]\n",
    "    sss_test = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "    test_subset_idx, _ = next(sss_test.split(np.zeros(len(test_targets)), test_targets))\n",
    "    test_DS = Subset(test_DS, test_subset_idx)\n",
    "\n",
    "    print(f\"[50% Subset] train: {len(train_DS)}개, val: {len(val_DS)}개, test: {len(test_DS)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf278e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_DS의 모든 라벨 추출\n",
    "if isinstance(train_DS, Subset):\n",
    "    targets = [train_DS.dataset.samples[i][1] for i in train_DS.indices]\n",
    "else:\n",
    "    targets = [lbl for _, lbl in train_DS.samples]\n",
    "\n",
    "# 클래스별 샘플 개수\n",
    "class_sample_count = np.bincount(targets)\n",
    "weights = 1. / torch.tensor(class_sample_count, dtype=torch.float)\n",
    "sample_weights = [weights[t] for t in targets]\n",
    "\n",
    "# WeightedRandomSampler 생성\n",
    "sampler = WeightedRandomSampler(\n",
    "    sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61ae47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DL = DataLoader(train_DS, batch_size=BATCH_SIZE, sampler=sampler, shuffle=False, num_workers=0, pin_memory=True)\n",
    "val_DL   = DataLoader(val_DS, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_DL  = DataLoader(test_DS, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba9e85",
   "metadata": {},
   "source": [
    "# Loss 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed9b76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=None, reduction='mean', num_classes=None):\n",
    "        \"\"\"\n",
    "        alpha: 초기 클래스별 가중치 리스트 또는 Tensor. None이면 균등 분포로 초기화.\n",
    "        gamma: focusing 파라미터\n",
    "        reduction: 'none' | 'mean' | 'sum'\n",
    "        num_classes: 클래스 수 (동적 기능 사용 시 반드시 지정)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "        if num_classes is None:\n",
    "            raise ValueError(\"동적 가중치 기능을 사용하려면 num_classes를 지정해야 합니다.\")\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # α 초기화\n",
    "        if alpha is not None:\n",
    "            a = torch.tensor(alpha, dtype=torch.float)\n",
    "            if a.numel() != num_classes:\n",
    "                raise ValueError(\"alpha 길이와 num_classes 불일치\")\n",
    "            self.register_buffer('alpha', a)\n",
    "        else:\n",
    "            init_alpha = torch.ones(num_classes, dtype=torch.float) / num_classes\n",
    "            self.register_buffer('alpha', init_alpha)\n",
    "\n",
    "        # 통계 누적용 버퍼\n",
    "        self.register_buffer('count', torch.zeros(num_classes, dtype=torch.long))\n",
    "        self.register_buffer('errors', torch.zeros(num_classes, dtype=torch.long))\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        inputs: (B, C) 로짓\n",
    "        targets: (B,) 정수 레이블\n",
    "        \"\"\"\n",
    "        # 1) cross-entropy 및 p_t 계산\n",
    "        ce = F.cross_entropy(inputs, targets, reduction='none')  # [B]\n",
    "        pt = torch.exp(-ce)                                      # [B]\n",
    "\n",
    "        # 2) α 및 focal term\n",
    "        at = self.alpha[targets]                                 # [B]\n",
    "        focal_term = (1 - pt).pow(self.gamma)                    # [B]\n",
    "\n",
    "        # 3) 최종 loss\n",
    "        loss = at * focal_term * ce                              # [B]\n",
    "\n",
    "        # 4) 통계 누적 (no grad)\n",
    "        with torch.no_grad():\n",
    "            # 클래스별 샘플 수\n",
    "            one_hot = F.one_hot(targets, self.num_classes).long()  # (B, C)\n",
    "            self.count += one_hot.sum(dim=0)\n",
    "\n",
    "            # 예측\n",
    "            preds = torch.argmax(inputs, dim=1)\n",
    "            # 클래스별 오분류 수\n",
    "            for cls in range(self.num_classes):\n",
    "                mask = (targets == cls)\n",
    "                if mask.any():\n",
    "                    self.errors[cls] += (preds[mask] != cls).sum()\n",
    "\n",
    "        # 5) reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:  # 'none'\n",
    "            return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_alpha(self,\n",
    "                     strategy: str = 'error',\n",
    "                     min_alpha: float = 0.05,\n",
    "                     max_alpha: float = 0.9):\n",
    "        \"\"\"\n",
    "        strategy: 'frequency' 또는 'error'\n",
    "        min_alpha, max_alpha: α를 이 범위 내로 클리핑\n",
    "        \"\"\"\n",
    "        # 1) 새로운 α 계산\n",
    "        if strategy == 'frequency':\n",
    "            inv_freq = 1.0 / (self.count.float() + 1e-12)\n",
    "            new_alpha = inv_freq / inv_freq.sum()\n",
    "        elif strategy == 'error':\n",
    "            err_rate = self.errors.float() / (self.count.float() + 1e-12)\n",
    "            new_alpha = err_rate / err_rate.sum()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "        # 2) 클리핑: α가 너무 작거나 크지 않도록 제한\n",
    "        new_alpha = new_alpha.clamp(min=min_alpha, max=max_alpha)\n",
    "\n",
    "        # 3) 재정규화: 합이 1이 되도록\n",
    "        new_alpha = new_alpha / new_alpha.sum()\n",
    "\n",
    "        # 4) buffer에 복사 & 통계 초기화\n",
    "        self.alpha.copy_(new_alpha)\n",
    "        self.reset_stats()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reset_stats(self):\n",
    "        \"\"\"count와 errors를 0으로 리셋\"\"\"\n",
    "        self.count.zero_()\n",
    "        self.errors.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bd171af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 개수: [267, 13041, 1128]\n",
      "초기화된 alpha: [0.7954335808753967, 0.016285618767142296, 0.18828082084655762]\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    init_alpha = [0.1, 0.2, 0.3]\n",
    "    num_classes = len(init_alpha)\n",
    "else:\n",
    "    # 1) train_DS에서 라벨만 추출 (Subset/원본 모두 대응)\n",
    "    if isinstance(train_DS, Subset):\n",
    "        labels = [train_DS.dataset.samples[i][1] for i in train_DS.indices]\n",
    "        num_classes = len(train_DS.dataset.classes)\n",
    "    else:\n",
    "        labels = [label for _, label in train_DS.samples]\n",
    "        num_classes = len(train_DS.classes)\n",
    "\n",
    "    # 2) 클래스별 개수 세기\n",
    "    counts = Counter(labels)\n",
    "    total_samples = sum(counts.values())\n",
    "\n",
    "    # 3) raw inverse-frequency 계산\n",
    "    raw = np.array(\n",
    "        [ total_samples / counts[i] for i in range(num_classes) ],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # 4) 정규화: 합이 1이 되도록\n",
    "    init_alpha = (raw / raw.sum()).tolist()\n",
    "\n",
    "    print(\"클래스별 개수:\", [counts[i] for i in range(num_classes)])\n",
    "    \n",
    "print(\"초기화된 alpha:\", init_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44a1e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222714f",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00bba731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(\n",
    "    model, train_DL, val_DL, criterion, optimizer,\n",
    "    EPOCH, BATCH_SIZE,\n",
    "    save_model_path, save_history_path,\n",
    "    # ReduceLROnPlateau 하이퍼파라미터\n",
    "    factor: float = 0.5,\n",
    "    patience: int = 3,\n",
    "    min_lr: float = 1e-5,\n",
    "    # Early Stopping 하이퍼파라미터\n",
    "    es_patience: int = 7,           # 개선 없을 때 몇 에폭 기다릴지\n",
    "):\n",
    "    scaler = GradScaler()\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode='min',\n",
    "        factor=factor, patience=patience,\n",
    "        min_lr=min_lr, verbose=True\n",
    "    )\n",
    "    \n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    acc_history  = {\n",
    "        'acc_train': [], 'acc_val': [],\n",
    "        'recall2_train': [], 'recall2_val': []\n",
    "    }\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0         # EarlyStopping 카운터\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        start = time.time()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"[Epoch {epoch+1}/{EPOCH}]  LR={current_lr:.2e}\")\n",
    "\n",
    "        # ── Train ──\n",
    "        model.train()\n",
    "        tr_loss, tr_acc, _, tr_recall2 = loss_epoch(\n",
    "            model, train_DL, criterion,\n",
    "            optimizer=optimizer, scheduler=None,\n",
    "            scaler=scaler, recall_class=2\n",
    "        )\n",
    "        loss_history['train'].append(tr_loss)\n",
    "        acc_history['acc_train'].append(tr_acc)\n",
    "        acc_history['recall2_train'].append(tr_recall2)\n",
    "\n",
    "        # ── Validation ──\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc, _, val_recall2 = loss_epoch(\n",
    "                model, val_DL, criterion,\n",
    "                optimizer=None, scheduler=None,\n",
    "                scaler=None, recall_class=2\n",
    "            )\n",
    "        loss_history['val'].append(val_loss)\n",
    "        acc_history['acc_val'].append(val_acc)\n",
    "        acc_history['recall2_val'].append(val_recall2)\n",
    "\n",
    "        # ReduceLROnPlateau\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # 모델 저장 및 EarlyStopping 체크\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'epoch': epoch+1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "            }, save_model_path)\n",
    "            print(\"Model saved! (improved val_loss)\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement for {epochs_no_improve}/{es_patience} epochs.\")\n",
    "\n",
    "        # 로그 출력\n",
    "        elapsed = timedelta(seconds=round(time.time() - start))\n",
    "        print(f\"  train loss {tr_loss:.4f} acc {tr_acc:.1f}% recall2 {tr_recall2:.1f}%\")\n",
    "        print(f\"    val loss {val_loss:.4f} acc {val_acc:.1f}% recall2 {val_recall2:.1f}%  time {elapsed}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        # W&B 기록\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train/loss': tr_loss,\n",
    "            'val/loss': val_loss,\n",
    "            'train/acc': tr_acc,\n",
    "            'val/acc': val_acc,\n",
    "            'train/recall2': tr_recall2,\n",
    "            'val/recall2': val_recall2,\n",
    "            'learning_rate': current_lr\n",
    "        })\n",
    "\n",
    "        # 동적 α 업데이트\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            criterion.update_alpha(strategy='error', min_alpha=0.05, max_alpha=0.9)\n",
    "            print(\"Updated alpha:\", criterion.alpha.cpu().numpy())\n",
    "\n",
    "        # ── Early Stopping 발동 조건 ──\n",
    "        if epochs_no_improve >= es_patience:\n",
    "            print(f\"Early stopping: validation loss has not improved for {es_patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    # 히스토리 저장\n",
    "    torch.save({\n",
    "        \"loss_history\": loss_history,\n",
    "        \"acc_history\" : acc_history,\n",
    "        \"EPOCH\": epoch + 1,          # 실제 학습한 epoch 수\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    }, save_history_path)\n",
    "\n",
    "    return loss_history, acc_history\n",
    "\n",
    "def loss_epoch(model, DL, criterion, optimizer=None, scheduler=None,\n",
    "               scaler=None, recall_class=2):\n",
    "    \"\"\"\n",
    "    AMP: scaler가 주어지면 autocast()와 scaler를 사용해 mixed precision 학습\n",
    "    \"\"\"\n",
    "    DEVICE = next(model.parameters()).device\n",
    "    N = len(DL.dataset)\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    true_pos = 0\n",
    "    actual_pos = 0\n",
    "\n",
    "    for x_batch, y_batch in tqdm(DL, leave=False):\n",
    "        x = x_batch.to(DEVICE)\n",
    "        y = y_batch.to(DEVICE)\n",
    "\n",
    "        # forward + loss (AMP 적용)\n",
    "        if scaler is not None:\n",
    "            # 수정: torch.amp.autocast 사용, device_type 명시\n",
    "            with autocast(device_type='cuda'):\n",
    "                y_hat = model(x)\n",
    "                raw = criterion(y_hat, y)\n",
    "                loss = raw[0] if isinstance(raw, tuple) else raw\n",
    "        else:\n",
    "            y_hat = model(x)\n",
    "            raw = criterion(y_hat, y)\n",
    "            loss = raw[0] if isinstance(raw, tuple) else raw\n",
    "\n",
    "        # backward & step\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        # accumulate loss & accuracy\n",
    "        batch_size = x.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = y_hat.argmax(dim=1)\n",
    "        total_correct += (preds == y).sum().item()\n",
    "\n",
    "        # recall 계산\n",
    "        mask = (y == recall_class)\n",
    "        actual_pos += mask.sum().item()\n",
    "        true_pos   += ((preds == recall_class) & mask).sum().item()\n",
    "\n",
    "    epoch_loss = total_loss / N\n",
    "    epoch_acc  = total_correct / N * 100\n",
    "    epoch_recall = (true_pos / actual_pos * 100) if actual_pos > 0 else 0.0\n",
    "\n",
    "    return epoch_loss, epoch_acc, total_correct, epoch_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47d9f6",
   "metadata": {},
   "source": [
    "# 하이퍼 파라미터 튜닝 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay_list = [1e-5, 1e-4, 1e-3]\n",
    "focal_gamma_list = [1.0, 1.5, 2.0, 2.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a73d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "results = []\n",
    "trial_num = 0\n",
    "\n",
    "for wd, fg in itertools.product(weight_decay_list, focal_gamma_list):\n",
    "    print(f\"\\n--- Grid Search: trial {trial_num} | weight_decay={wd}, focal_gamma={fg} ---\")\n",
    "\n",
    "    wandb.init(\n",
    "        project=f\"Scalp_Disease_Classification_{DESEASE}_grid\",\n",
    "        name=f\"trial_{trial_num}_wd{wd:.1e}_fg{fg:.2f}\",\n",
    "        config={\"lr\": LR, \"weight_decay\": wd, \"focal_gamma\": fg},\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(512, 3)\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    criterion = DynamicFocalLoss(alpha=init_alpha, gamma=fg, reduction='mean', num_classes=3).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=wd)\n",
    "    \n",
    "    # train\n",
    "    _, acc_history = Train(model, train_DL, val_DL, criterion, optimizer,\n",
    "                           EPOCH, BATCH_SIZE, save_model_path, save_history_path)\n",
    "    wandb.finish()\n",
    "    val_acc = acc_history['acc_val'][-1]\n",
    "    results.append({\n",
    "        \"trial\": trial_num,\n",
    "        \"weight_decay\": wd,\n",
    "        \"focal_gamma\": fg,\n",
    "        \"val_acc\": val_acc\n",
    "    })\n",
    "    trial_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results.sort_values(by='val_acc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6db53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_row = df_results.loc[df_results['val_acc'].idxmax()]\n",
    "best_weight_decay = best_row['weight_decay']\n",
    "best_focal_gamma  = best_row['focal_gamma']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797b494",
   "metadata": {},
   "source": [
    "# 재학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44d7a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 데이터 다시 불러오기\n",
    "# 전체 데이터셋 (test 폴더에서 전체 샘플 가져옴)\n",
    "full_ds = CustomOrderImageFolder(\n",
    "    root = os.path.join(data_path, 'test'),\n",
    "    transform = None,                  # 무시되므로 None 으로\n",
    "    custom_order = custom_order,\n",
    ")\n",
    "\n",
    "# Stratified Split (앞에서 설명된 대로)\n",
    "labels = np.array([lbl for _, lbl in full_ds.samples])\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "train_idx, test_idx = next(sss.split(np.zeros(len(labels)), labels))\n",
    "\n",
    "train_DS = CustomOrderImageFolder(\n",
    "    root = os.path.join(data_path, 'train'),\n",
    "    transform = train_transform,\n",
    "    custom_order = custom_order,\n",
    ")\n",
    "\n",
    "val_DS = CustomSubset(\n",
    "    image_folder=full_ds,\n",
    "    indices=train_idx,\n",
    "    transform=test_val_transform\n",
    ")\n",
    "test_DS = CustomSubset(\n",
    "    image_folder=full_ds,\n",
    "    indices=test_idx,\n",
    "    transform=test_val_transform\n",
    ")\n",
    "\n",
    "# train_DS의 모든 라벨 추출\n",
    "if isinstance(train_DS, Subset):\n",
    "    targets = [train_DS.dataset.samples[i][1] for i in train_DS.indices]\n",
    "else:\n",
    "    targets = [lbl for _, lbl in train_DS.samples]\n",
    "\n",
    "# 클래스별 샘플 개수\n",
    "class_sample_count = np.bincount(targets)\n",
    "weights = 1. / torch.tensor(class_sample_count, dtype=torch.float)\n",
    "sample_weights = [weights[t] for t in targets]\n",
    "\n",
    "# WeightedRandomSampler 생성\n",
    "sampler = WeightedRandomSampler(\n",
    "    sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_DL = DataLoader(train_DS, batch_size=BATCH_SIZE, sampler=sampler, shuffle=False, num_workers=0, pin_memory=True)\n",
    "val_DL   = DataLoader(val_DS, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_DL  = DataLoader(test_DS, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae33b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "in_features = final_model.classifier[1].in_features\n",
    "final_model.classifier[1] = nn.Sequential(\n",
    "    nn.Linear(in_features, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(512, 3)\n",
    ")\n",
    "final_model = final_model.to(DEVICE)\n",
    "\n",
    "final_criterion = DynamicFocalLoss(\n",
    "    alpha=init_alpha,\n",
    "    # gamma=best_focal_gamma,\n",
    "    # gamma=best_focal_gamma,\n",
    "    gamma=2.0,  # 고정된 gamma 사용\n",
    "    reduction='mean',\n",
    "    num_classes=3\n",
    ").to(DEVICE)\n",
    "\n",
    "final_optimizer = torch.optim.Adam(\n",
    "    final_model.parameters(), lr=LR, weight_decay=1e-4  # 고정된 weight_decay 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca594d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\user1\\Desktop\\Code\\Scalp_Disease_Classifier\\code\\biddem\\wandb\\run-20250530_032004-lbsr676v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rmfth0438/Scalp_Disease_Classification_BIDDEM/runs/lbsr676v' target=\"_blank\">FINAL_MODEL_BIDDEM</a></strong> to <a href='https://wandb.ai/rmfth0438/Scalp_Disease_Classification_BIDDEM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rmfth0438/Scalp_Disease_Classification_BIDDEM' target=\"_blank\">https://wandb.ai/rmfth0438/Scalp_Disease_Classification_BIDDEM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rmfth0438/Scalp_Disease_Classification_BIDDEM/runs/lbsr676v' target=\"_blank\">https://wandb.ai/rmfth0438/Scalp_Disease_Classification_BIDDEM/runs/lbsr676v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rmfth0438/Scalp_Disease_Classification_BIDDEM/runs/lbsr676v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1229d79d1d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. wandb로 최종 학습 기록 시작 (실험명 구분)\n",
    "wandb.init(\n",
    "    project=f\"Scalp_Disease_Classification_{DESEASE}\",\n",
    "    name=f\"FINAL_MODEL_{DESEASE}\",\n",
    "    config={\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": best_weight_decay,\n",
    "        \"focal_gamma\": best_focal_gamma,\n",
    "        \"epochs\": 50\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4467a1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_8068\\3905160355.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "c:\\Users\\user1\\anaconda3\\envs\\pt_py311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved! (improved val_loss)\n",
      "  train loss 0.0322 acc 66.8% recall2 88.3%\n",
      "    val loss 0.0192 acc 18.8% recall2 89.8%  time 0:03:13\n",
      "------------------------------------------------------------\n",
      "[Epoch 2/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved! (improved val_loss)\n",
      "  train loss 0.0109 acc 70.7% recall2 97.5%\n",
      "    val loss 0.0178 acc 24.2% recall2 94.4%  time 0:03:14\n",
      "------------------------------------------------------------\n",
      "[Epoch 3/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved! (improved val_loss)\n",
      "  train loss 0.0081 acc 74.9% recall2 98.5%\n",
      "    val loss 0.0155 acc 32.8% recall2 96.0%  time 0:03:13\n",
      "------------------------------------------------------------\n",
      "[Epoch 4/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved! (improved val_loss)\n",
      "  train loss 0.0067 acc 78.0% recall2 98.8%\n",
      "    val loss 0.0135 acc 41.1% recall2 95.7%  time 0:03:13\n",
      "------------------------------------------------------------\n",
      "[Epoch 5/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved! (improved val_loss)\n",
      "  train loss 0.0057 acc 80.5% recall2 99.1%\n",
      "    val loss 0.0120 acc 46.4% recall2 96.6%  time 0:03:12\n",
      "------------------------------------------------------------\n",
      "[Epoch 6/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1/7 epochs.\n",
      "  train loss 0.0050 acc 83.1% recall2 99.2%\n",
      "    val loss 0.0124 acc 49.2% recall2 92.9%  time 0:03:12\n",
      "------------------------------------------------------------\n",
      "[Epoch 7/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved! (improved val_loss)\n",
      "  train loss 0.0045 acc 84.4% recall2 99.3%\n",
      "    val loss 0.0094 acc 58.6% recall2 95.3%  time 0:03:13\n",
      "------------------------------------------------------------\n",
      "[Epoch 8/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved! (improved val_loss)\n",
      "  train loss 0.0041 acc 85.8% recall2 99.3%\n",
      "    val loss 0.0083 acc 65.1% recall2 91.6%  time 0:03:12\n",
      "------------------------------------------------------------\n",
      "[Epoch 9/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1/7 epochs.\n",
      "  train loss 0.0039 acc 86.9% recall2 99.4%\n",
      "    val loss 0.0091 acc 62.6% recall2 93.5%  time 0:03:12\n",
      "------------------------------------------------------------\n",
      "[Epoch 10/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved! (improved val_loss)\n",
      "  train loss 0.0034 acc 88.1% recall2 99.6%\n",
      "    val loss 0.0082 acc 67.2% recall2 92.5%  time 0:03:11\n",
      "------------------------------------------------------------\n",
      "Updated alpha: [0.05 0.9  0.05]\n",
      "[Epoch 11/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1/7 epochs.\n",
      "  train loss 0.0227 acc 82.9% recall2 58.0%\n",
      "    val loss 0.0377 acc 93.0% recall2 43.8%  time 0:03:11\n",
      "------------------------------------------------------------\n",
      "[Epoch 12/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 2/7 epochs.\n",
      "  train loss 0.0157 acc 81.6% recall2 53.6%\n",
      "    val loss 0.0273 acc 93.4% recall2 43.2%  time 0:03:12\n",
      "------------------------------------------------------------\n",
      "[Epoch 13/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 3/7 epochs.\n",
      "  train loss 0.0137 acc 82.3% recall2 53.5%\n",
      "    val loss 0.0336 acc 93.3% recall2 46.6%  time 0:03:11\n",
      "------------------------------------------------------------\n",
      "[Epoch 14/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 4/7 epochs.\n",
      "  train loss 0.0127 acc 83.2% recall2 55.7%\n",
      "    val loss 0.0250 acc 93.3% recall2 38.8%  time 0:03:11\n",
      "------------------------------------------------------------\n",
      "[Epoch 15/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 5/7 epochs.\n",
      "  train loss 0.0117 acc 84.7% recall2 60.1%\n",
      "    val loss 0.0311 acc 93.2% recall2 50.0%  time 0:03:11\n",
      "------------------------------------------------------------\n",
      "[Epoch 16/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 6/7 epochs.\n",
      "  train loss 0.0105 acc 85.9% recall2 64.0%\n",
      "    val loss 0.0268 acc 93.0% recall2 42.9%  time 0:03:11\n",
      "------------------------------------------------------------\n",
      "[Epoch 17/50]  LR=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 7/7 epochs.\n",
      "  train loss 0.0094 acc 87.1% recall2 65.9%\n",
      "    val loss 0.0240 acc 93.1% recall2 39.1%  time 0:03:12\n",
      "------------------------------------------------------------\n",
      "Early stopping: validation loss has not improved for 7 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# 4. 전체 epoch(예: 50) 재학습\n",
    "EPOCH = 50  # 또는 원하는 epoch 수\n",
    "save_model_path = rf\"C:\\Users\\user1\\Desktop\\Code\\Scalp_Disease_Classifier\\result\\model\\{model_type}_final_model.pt\"\n",
    "save_history_path = rf\"C:\\Users\\user1\\Desktop\\Code\\Scalp_Disease_Classifier\\result\\history\\{model_type}_final_history.pt\"\n",
    "\n",
    "final_loss_history, final_acc_history = Train(\n",
    "    final_model, train_DL, val_DL, final_criterion, final_optimizer,\n",
    "    EPOCH, BATCH_SIZE, save_model_path, save_history_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3db219",
   "metadata": {},
   "source": [
    "# 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b432e",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m save_history_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124muser1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mScalp_Disease_Classifier\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mEfficientNet_B0_BIDDEM_TUNNING_2025_06_05_12_final_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_history_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_history\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user1\\anaconda3\\envs\\pt_py311\\Lib\\site-packages\\torch\\serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1464\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1468\u001b[0m                 )\n\u001b[0;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1472\u001b[0m             opened_zipfile,\n\u001b[0;32m   1473\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(save_history_path, map_location=DEVICE)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1, checkpoint['EPOCH'] + 1), checkpoint['loss_history']['train'], label='Train Loss', marker='o')\n",
    "plt.plot(range(1, checkpoint['EPOCH'] + 1), checkpoint['loss_history']['val'], label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0dccd78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43mcheckpoint\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_history\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_train\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain ACC\u001b[39m\u001b[38;5;124m'\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_history\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_val\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation ACC\u001b[39m\u001b[38;5;124m'\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1, checkpoint['EPOCH'] + 1), checkpoint['acc_history']['acc_train'], label='Train ACC', marker='o')\n",
    "plt.plot(range(1, checkpoint['EPOCH'] + 1), checkpoint['acc_history']['acc_val'], label='Validation ACC', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation ACC')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80f60748",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43mcheckpoint\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_history\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall2_train\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain RECALL\u001b[39m\u001b[38;5;124m'\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_history\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall2_val\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation RECALL\u001b[39m\u001b[38;5;124m'\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1, checkpoint['EPOCH'] + 1), checkpoint['acc_history']['recall2_train'], label='Train RECALL', marker='o')\n",
    "plt.plot(range(1, checkpoint['EPOCH'] + 1), checkpoint['acc_history']['recall2_val'], label='Validation RECALL', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation RECALL')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c78167",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0af4b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def Test(model, test_DL, criterion, num_classes, recall_class=2):\n",
    "    \"\"\"\n",
    "    모델을 평가하고, 손실/정확도/리콜 지표와\n",
    "    클래스별 정확도, classification report,\n",
    "    그리고 실제 vs 예측 레이블을 반환합니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    DEVICE = next(model.parameters()).device\n",
    "\n",
    "    total_loss   = 0.0\n",
    "    total_correct= 0\n",
    "    true_pos     = 0\n",
    "    actual_pos   = 0\n",
    "    all_preds    = []\n",
    "    all_labels   = []\n",
    "\n",
    "    class_correct = [0] * num_classes\n",
    "    class_total   = [0] * num_classes\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in tqdm(test_DL, desc=\"Testing\"):\n",
    "            x = x_batch.to(DEVICE)\n",
    "            y = y_batch.to(DEVICE)\n",
    "\n",
    "            # 순전파 + 손실 계산\n",
    "            y_hat = model(x)\n",
    "            raw   = criterion(y_hat, y)\n",
    "            loss  = raw if not isinstance(raw, (tuple,list)) else raw[0]\n",
    "\n",
    "            batch_size = x.size(0)\n",
    "            total_loss    += loss.item() * batch_size\n",
    "            preds         = y_hat.argmax(dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "\n",
    "            # 클래스별 correct/total 집계\n",
    "            for label, pred in zip(y.cpu().tolist(), preds.cpu().tolist()):\n",
    "                class_total[label]   += 1\n",
    "                if pred == label:\n",
    "                    class_correct[label] += 1\n",
    "\n",
    "            # 특정 클래스(recall_class) 리콜 집계\n",
    "            mask         = (y == recall_class)\n",
    "            actual_pos  += mask.sum().item()\n",
    "            true_pos    += ((preds == recall_class) & mask).sum().item()\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "    N = len(test_DL.dataset)\n",
    "    test_loss   = total_loss / N\n",
    "    test_acc    = total_correct / N * 100\n",
    "    test_recall = (true_pos / actual_pos * 100) if actual_pos > 0 else 0.0\n",
    "\n",
    "    # 클래스별 정확도 계산 (%)\n",
    "    class_accs = [\n",
    "        (class_correct[i] / class_total[i] * 100) if class_total[i] > 0 else 0.0\n",
    "        for i in range(num_classes)\n",
    "    ]\n",
    "\n",
    "    # classification report 생성\n",
    "    report_dict = classification_report(\n",
    "        all_labels, all_preds,\n",
    "        output_dict=True, digits=4\n",
    "    )\n",
    "\n",
    "    return test_loss, test_acc, test_recall, class_accs, report_dict, all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aff412cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 129/129 [00:20<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:      0.0070\n",
      "Accuracy:  84.1%\n",
      "Recall(2): 82.0%\n",
      "Precision (macro): 54.24%\n",
      "F1 Score   (macro): 60.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_model_path = r'C:\\Users\\user1\\Desktop\\Code\\Scalp_Disease_Classifier\\result\\model\\EfficientNet_B0_BIDDEM_TUNNING_2025_06_05_12_final_model.pt'\n",
    "\n",
    "model = efficientnet_b0()  # 사용자 정의 모델 클래스\n",
    "in_features = model.classifier[1].in_features  # 보통 1792\n",
    "model.classifier[1] = nn.Sequential(nn.Linear(in_features, 512),\n",
    "                                    nn.BatchNorm1d(512),       # 배치 정규화 추가\n",
    "                                    nn.ReLU(),                 # 혹은 nn.ReLU()\n",
    "                                    nn.Linear(512, 3))         # 최종 클래스 수\n",
    "model.load_state_dict(torch.load(save_model_path, weights_only=False)['model'])\n",
    "model.to(DEVICE)\n",
    "\n",
    "test_loss, test_acc, test_recall, class_accs, report_dict, all_labels, all_preds = Test(model, test_DL, final_criterion, num_classes, recall_class=2)\n",
    "\n",
    "# 2) 매크로 평균 정밀도·F1 추출\n",
    "macro = report_dict['macro avg']\n",
    "precision = macro['precision'] * 100\n",
    "f1_score  = macro['f1-score'] * 100\n",
    "\n",
    "# 3) 깔끔한 출력\n",
    "print(f\"Loss:      {test_loss:.4f}\")\n",
    "print(f\"Accuracy:  {test_acc:.1f}%\")\n",
    "print(f\"Recall(2): {test_recall:.1f}%\")\n",
    "print(f\"Precision (macro): {precision:.2f}%\")\n",
    "print(f\"F1 Score   (macro): {f1_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2c5f9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>ACC</th>\n",
       "      <th>ACC_양호</th>\n",
       "      <th>ACC_경증</th>\n",
       "      <th>ACC_중증</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EfficientNet_B4_BIDDEM_V1.1_2025_05_19_12</td>\n",
       "      <td>64.21</td>\n",
       "      <td>40.12</td>\n",
       "      <td>45.20</td>\n",
       "      <td>100.00</td>\n",
       "      <td>29.92</td>\n",
       "      <td>64.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EfficientNet_B4_BIDDEM_V1.1_2025_05_19_14</td>\n",
       "      <td>64.21</td>\n",
       "      <td>40.12</td>\n",
       "      <td>45.20</td>\n",
       "      <td>100.00</td>\n",
       "      <td>29.92</td>\n",
       "      <td>64.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EfficientNet_B4_BIDDEM_V1.1_2025_05_19_14</td>\n",
       "      <td>64.21</td>\n",
       "      <td>40.12</td>\n",
       "      <td>45.20</td>\n",
       "      <td>100.00</td>\n",
       "      <td>29.92</td>\n",
       "      <td>64.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EfficientNet_B3_BIDDEM_V1_2025_05_19_16</td>\n",
       "      <td>100.00</td>\n",
       "      <td>19.31</td>\n",
       "      <td>40.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EfficientNet_B3_BIDDEM_V1.1_2025_05_19_17</td>\n",
       "      <td>100.00</td>\n",
       "      <td>19.31</td>\n",
       "      <td>40.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EfficientNet_B3_BIDDEM_V1.1_2025_05_20_15</td>\n",
       "      <td>100.00</td>\n",
       "      <td>19.31</td>\n",
       "      <td>40.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EfficientNet_B3_BIDDEM_V1.1_2025_05_20_15</td>\n",
       "      <td>100.00</td>\n",
       "      <td>19.31</td>\n",
       "      <td>40.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EfficientNet_B3_BIDDEM_V1.1_2025_05_20_15</td>\n",
       "      <td>79.55</td>\n",
       "      <td>60.11</td>\n",
       "      <td>72.62</td>\n",
       "      <td>92.11</td>\n",
       "      <td>67.08</td>\n",
       "      <td>79.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EfficientNet_B3_BIDDEM_V1.2_2025_05_20_22</td>\n",
       "      <td>49.41</td>\n",
       "      <td>63.39</td>\n",
       "      <td>73.64</td>\n",
       "      <td>82.89</td>\n",
       "      <td>90.57</td>\n",
       "      <td>49.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EfficientNet_B3_BIDDEM_V3.1_2025_05_24_19</td>\n",
       "      <td>90.99</td>\n",
       "      <td>55.21</td>\n",
       "      <td>77.64</td>\n",
       "      <td>93.42</td>\n",
       "      <td>76.17</td>\n",
       "      <td>90.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EfficientNet_B3_BIDDEM_V3.2_2025_05_26_12</td>\n",
       "      <td>90.99</td>\n",
       "      <td>50.92</td>\n",
       "      <td>72.77</td>\n",
       "      <td>89.47</td>\n",
       "      <td>70.85</td>\n",
       "      <td>90.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>EfficientNet_B3_BIDDEM_V4_2025_05_26_17</td>\n",
       "      <td>88.82</td>\n",
       "      <td>52.82</td>\n",
       "      <td>75.19</td>\n",
       "      <td>93.42</td>\n",
       "      <td>73.64</td>\n",
       "      <td>88.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EfficientNet_B3_BIDDEM_V4.1_2025_05_28_16</td>\n",
       "      <td>86.96</td>\n",
       "      <td>47.88</td>\n",
       "      <td>67.51</td>\n",
       "      <td>98.68</td>\n",
       "      <td>65.19</td>\n",
       "      <td>86.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EfficientNet_B3_BIDDEM_V5_TUNNING_2025_05_29_18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.64</td>\n",
       "      <td>90.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EfficientNet_B3_BIDDEM_V5_TUNNING_2025_05_29_20</td>\n",
       "      <td>92.86</td>\n",
       "      <td>50.38</td>\n",
       "      <td>71.80</td>\n",
       "      <td>86.84</td>\n",
       "      <td>69.67</td>\n",
       "      <td>92.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>EfficientNet_B0_BIDDEM_V1_2025_06_04_22</td>\n",
       "      <td>88.51</td>\n",
       "      <td>57.90</td>\n",
       "      <td>80.07</td>\n",
       "      <td>85.53</td>\n",
       "      <td>79.23</td>\n",
       "      <td>88.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>EfficientNet_B0_BIDDEM_V1_2025_06_04_23</td>\n",
       "      <td>88.51</td>\n",
       "      <td>57.90</td>\n",
       "      <td>80.07</td>\n",
       "      <td>85.53</td>\n",
       "      <td>79.23</td>\n",
       "      <td>88.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>EfficientNet_B0_BIDDEM_TUNNING_2025_06_05_15</td>\n",
       "      <td>81.99</td>\n",
       "      <td>60.86</td>\n",
       "      <td>84.07</td>\n",
       "      <td>80.26</td>\n",
       "      <td>84.33</td>\n",
       "      <td>81.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Model  Recall     F1    ACC  \\\n",
       "0         EfficientNet_B4_BIDDEM_V1.1_2025_05_19_12   64.21  40.12  45.20   \n",
       "1         EfficientNet_B4_BIDDEM_V1.1_2025_05_19_14   64.21  40.12  45.20   \n",
       "2         EfficientNet_B4_BIDDEM_V1.1_2025_05_19_14   64.21  40.12  45.20   \n",
       "3           EfficientNet_B3_BIDDEM_V1_2025_05_19_16  100.00  19.31  40.79   \n",
       "4         EfficientNet_B3_BIDDEM_V1.1_2025_05_19_17  100.00  19.31  40.79   \n",
       "5         EfficientNet_B3_BIDDEM_V1.1_2025_05_20_15  100.00  19.31  40.79   \n",
       "6         EfficientNet_B3_BIDDEM_V1.1_2025_05_20_15  100.00  19.31  40.79   \n",
       "7         EfficientNet_B3_BIDDEM_V1.1_2025_05_20_15   79.55  60.11  72.62   \n",
       "8         EfficientNet_B3_BIDDEM_V1.2_2025_05_20_22   49.41  63.39  73.64   \n",
       "9         EfficientNet_B3_BIDDEM_V3.1_2025_05_24_19   90.99  55.21  77.64   \n",
       "10        EfficientNet_B3_BIDDEM_V3.2_2025_05_26_12   90.99  50.92  72.77   \n",
       "11          EfficientNet_B3_BIDDEM_V4_2025_05_26_17   88.82  52.82  75.19   \n",
       "12        EfficientNet_B3_BIDDEM_V4.1_2025_05_28_16   86.96  47.88  67.51   \n",
       "13  EfficientNet_B3_BIDDEM_V5_TUNNING_2025_05_29_18    0.00  31.64  90.35   \n",
       "14  EfficientNet_B3_BIDDEM_V5_TUNNING_2025_05_29_20   92.86  50.38  71.80   \n",
       "15          EfficientNet_B0_BIDDEM_V1_2025_06_04_22   88.51  57.90  80.07   \n",
       "16          EfficientNet_B0_BIDDEM_V1_2025_06_04_23   88.51  57.90  80.07   \n",
       "17     EfficientNet_B0_BIDDEM_TUNNING_2025_06_05_15   81.99  60.86  84.07   \n",
       "\n",
       "    ACC_양호  ACC_경증  ACC_중증  \n",
       "0   100.00   29.92   64.21  \n",
       "1   100.00   29.92   64.21  \n",
       "2   100.00   29.92   64.21  \n",
       "3     0.00    0.00  100.00  \n",
       "4     0.00    0.00  100.00  \n",
       "5     0.00    0.00  100.00  \n",
       "6     0.00    0.00  100.00  \n",
       "7    92.11   67.08   79.55  \n",
       "8    82.89   90.57   49.41  \n",
       "9    93.42   76.17   90.99  \n",
       "10   89.47   70.85   90.99  \n",
       "11   93.42   73.64   88.82  \n",
       "12   98.68   65.19   86.96  \n",
       "13    0.00  100.00    0.00  \n",
       "14   86.84   69.67   92.86  \n",
       "15   85.53   79.23   88.51  \n",
       "16   85.53   79.23   88.51  \n",
       "17   80.26   84.33   81.99  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_macro = report_dict['macro avg']['f1-score'] * 100  # % 단위로 변환\n",
    "\n",
    "# 저장할 딕셔너리 구성\n",
    "data = {\n",
    "    \"Model\": [model_type],\n",
    "    \"Recall\":[test_recall],\n",
    "    \"F1\":    [f1_macro],           # Macro F1 추가\n",
    "    \"ACC\":   [test_acc],\n",
    "}\n",
    "\n",
    "# 클래스별 ACC 컬럼 추가\n",
    "class_names = [\"양호\", \"경증\", \"중증\"]\n",
    "for i, acc in enumerate(class_accs):\n",
    "    data[f\"ACC_{class_names[i]}\"] = [acc]\n",
    "\n",
    "# DataFrame 생성 및 CSV에 append/저장\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.round(2)\n",
    "\n",
    "if os.path.isfile(save_result_csv_path):\n",
    "    df.to_csv(save_result_csv_path, mode=\"a\", header=False, index=False)\n",
    "else:\n",
    "    df.to_csv(save_result_csv_path, index=False)\n",
    "\n",
    "# 결과 확인\n",
    "df = pd.read_csv(save_result_csv_path)\n",
    "df = df.round(2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da916de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Normalized Confusion Matrix (rows sum to 1) ===\n",
      "        Pred_0  Pred_1  Pred_2\n",
      "True_0    0.80    0.20    0.00\n",
      "True_1    0.05    0.84    0.11\n",
      "True_2    0.00    0.18    0.82\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ── 혼동 행렬 계산 및 정규화 ──\n",
    "cm      = confusion_matrix(all_labels, all_preds)\n",
    "cm_norm = cm.astype(np.float32) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "# DataFrame으로 보기 좋게 정리\n",
    "index   = [f\"True_{i}\" for i in range(num_classes)]\n",
    "columns = [f\"Pred_{i}\" for i in range(num_classes)]\n",
    "df_cm   = pd.DataFrame(cm_norm, index=index, columns=columns)\n",
    "df_cm = df_cm.round(2)\n",
    "print(\"\\n=== Normalized Confusion Matrix (rows sum to 1) ===\")\n",
    "print(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc689b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a63aecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
